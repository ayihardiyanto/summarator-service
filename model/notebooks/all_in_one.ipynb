{
 "cells": [
  {
   "source": [
    "## Initiate Text Sample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = '../text_data/input.txt'\n",
    "# file = open(file , 'r')\n",
    "# text = file.read()"
   ]
  },
  {
   "source": [
    "# Performing TD-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing Modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import operator\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances_argmin_min\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial import distance\n",
    "from string import punctuation"
   ]
  },
  {
   "source": [
    "### Downloading libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/hackintosh/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/hackintosh/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/hackintosh/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/hackintosh/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stopwords = set(stopwords.words('english'))\n",
    "wordlemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(words):\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "       lemmatized_words.append(wordlemmatizer.lemmatize(word))\n",
    "    return lemmatized_words\n",
    "def stem_words(words):\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "       stemmed_words.append(PorterStemmer().stem(word))\n",
    "    return stemmed_words\n",
    "def remove_special_characters(text):\n",
    "    regex = r'[^a-zA-Z0-9\\s]'\n",
    "    text = re.sub(regex,'',text)\n",
    "    return text\n",
    "def freq(words):\n",
    "    words = [word.lower() for word in words]\n",
    "    dict_freq = {}\n",
    "    words_unique = []\n",
    "    for word in words:\n",
    "       if word not in words_unique:\n",
    "           words_unique.append(word)\n",
    "    for word in words_unique:\n",
    "       dict_freq[word] = words.count(word)\n",
    "    return dict_freq\n",
    "def pos_tagging(text):\n",
    "    pos_tag = nltk.pos_tag(text.split())\n",
    "    pos_tagged_noun_verb = []\n",
    "    for word,tag in pos_tag:\n",
    "        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n",
    "             pos_tagged_noun_verb.append(word)\n",
    "    return pos_tagged_noun_verb\n",
    "def tf_score(word,sentence):\n",
    "    freq_sum = 0\n",
    "    word_frequency_in_sentence = 0\n",
    "    len_sentence = len(sentence)\n",
    "    for word_in_sentence in sentence.split():\n",
    "        if word == word_in_sentence:\n",
    "            word_frequency_in_sentence = word_frequency_in_sentence + 1\n",
    "    tf =  word_frequency_in_sentence/ len_sentence\n",
    "    return tf\n",
    "def idf_score(no_of_sentences,word,sentences):\n",
    "    no_of_sentence_containing_word = 0\n",
    "    # print('WORDS', word)\n",
    "    for sentence in sentences:\n",
    "        sentence = remove_special_characters(str(sentence))\n",
    "        sentence = re.sub(r'\\d+', '', sentence)\n",
    "        sentence = sentence.split()\n",
    "        sentence = [word for word in sentence if word.lower() not in Stopwords and len(word)>1]\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "        sentence = [wordlemmatizer.lemmatize(word) for word in sentence]\n",
    "        if word in sentence:\n",
    "            no_of_sentence_containing_word = no_of_sentence_containing_word + 1\n",
    "    idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\n",
    "    return idf\n",
    "def tf_idf_score(tf,idf):\n",
    "    return tf*idf\n",
    "def word_tfidf(dict_freq,word,sentences,sentence):\n",
    "    word_tfidf = []\n",
    "    tf = tf_score(word,sentence)\n",
    "    idf = idf_score(len(sentences),word,sentences)\n",
    "    tf_idf = tf_idf_score(tf,idf)\n",
    "    return tf_idf\n",
    "def sentence_importance(sentence,dict_freq,sentences):\n",
    "     sentence_score = 0\n",
    "     sentence = remove_special_characters(str(sentence)) \n",
    "     sentence = re.sub(r'\\d+', '', sentence)\n",
    "     pos_tagged_sentence = [] \n",
    "     no_of_sentences = len(sentences)\n",
    "     pos_tagged_sentence = pos_tagging(sentence)\n",
    "     for word in pos_tagged_sentence:\n",
    "          if word.lower() not in Stopwords and word not in Stopwords and len(word)>1: \n",
    "                word = word.lower()\n",
    "                word = wordlemmatizer.lemmatize(word)\n",
    "                sentence_score = sentence_score + word_tfidf(dict_freq,word,sentences,sentence)\n",
    "     return sentence_score\n",
    "# file = '../text_data/input.txt'\n",
    "# file = open(file , 'r')\n",
    "# text = file.read()\n",
    "def buildTfIdfSummary(text, retention_rate):\n",
    "    summary = []\n",
    "    sentence_no = []\n",
    "\n",
    "    tokenized_sentence = sent_tokenize(text.rstrip('\\n'))\n",
    "    text = remove_special_characters(str(text))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    tokenized_words_with_stopwords = word_tokenize(text)\n",
    "    tokenized_words = [word for word in tokenized_words_with_stopwords if word not in Stopwords]\n",
    "    tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "    tokenized_words = [word.lower() for word in tokenized_words]\n",
    "    tokenized_words = lemmatize_words(tokenized_words)\n",
    "\n",
    "\n",
    "    sentence_with_importance = getSentenceImportance(tokenized_sentence, tokenized_words)\n",
    "\n",
    "    sentence_with_importance = sorted(sentence_with_importance.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "    addSentenceNumber(tokenized_sentence, sentence_with_importance, sentence_no, retention_rate)\n",
    "    \n",
    "    sentence_no.sort()\n",
    "\n",
    "    summary = assembleWords(tokenized_sentence, sentence_no)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def getSentenceImportance(tokenized_sentence, tokenized_words):\n",
    "    sentence_with_importance = {}\n",
    "    c = 1\n",
    "    word_freq = freq(tokenized_words)\n",
    "    for sent in tokenized_sentence:\n",
    "        sentenceimp = sentence_importance(sent,word_freq,tokenized_sentence)\n",
    "        sentence_with_importance[c] = sentenceimp\n",
    "        c += 1\n",
    "    return sentence_with_importance\n",
    "\n",
    "\n",
    "def addSentenceNumber(tokenized_sentence, sentence_with_importance, sentence_no, retention_rate):\n",
    "    cnt = 0\n",
    "    no_of_sentences = int((retention_rate * len(tokenized_sentence))/100)\n",
    "    for word_prob in sentence_with_importance:\n",
    "        if cnt < no_of_sentences:\n",
    "            sentence_no.append(word_prob[0])\n",
    "            cnt = cnt+1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "def assembleWords(tokenized_sentence, sentence_no):\n",
    "    cnt = 1\n",
    "    summary = []\n",
    "    for sentence in tokenized_sentence:\n",
    "        if cnt in sentence_no:\n",
    "            summary.append(sentence)\n",
    "        cnt = cnt+1\n",
    "    \n",
    "    summary = \" \".join(summary)\n",
    "    return summary\n"
   ]
  },
  {
   "source": [
    "## Word Vectorizing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineCorpus(sentence):\n",
    "    corpus = []\n",
    "    for i in range(len(sentence)):\n",
    "        sen = re.sub('[^a-zA-Z]', \" \", sentence[i])\n",
    "        sen = sen.lower()\n",
    "        sen = sen.split()\n",
    "        sen = ' '.join([i for i in sen if i not in stopwords.words('english')])\n",
    "        corpus.append(sen)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def vectorize(corpus):\n",
    "\n",
    "    # creating word vectors\n",
    "    n = 300\n",
    "    all_words = [i.split() for i in corpus]\n",
    "    model = Word2Vec(all_words, min_count=1, size=n)\n",
    "\n",
    "    # creating sentence vectors\n",
    "    sen_vector = []\n",
    "    for i in corpus:\n",
    "        plus = 0\n",
    "        for j in i.split():\n",
    "            plus += model.wv[j]\n",
    "        plus = plus/len(plus)\n",
    "        sen_vector.append(plus)\n",
    "    return sen_vector"
   ]
  },
  {
   "source": [
    "## K-Means"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performKMeans(text, cluster):\n",
    "    sentence = sent_tokenize(text)\n",
    "    corpus = defineCorpus(sentence)\n",
    "    sentenceVector = vectorize(corpus)\n",
    "    result = kMeansClustering(sentenceVector, cluster)\n",
    "    summary = []\n",
    "    for i in sorted(result):\n",
    "        summary.append(sentence[i])\n",
    "    summary = \" \".join(summary)\n",
    "    return summary\n",
    "\n",
    "# performing k-means\n",
    "\n",
    "\n",
    "def kMeansClustering(sen_vector, cluster):\n",
    "    n_clusters = cluster\n",
    "    kmeans = KMeans(n_clusters, init='k-means++', random_state=43)\n",
    "    y_kmeans = kmeans.fit_predict(sen_vector)\n",
    "\n",
    "    # finding and printing the nearest sentence vector from cluster centroid\n",
    "    my_list = []\n",
    "    for i in range(n_clusters):\n",
    "        my_dict = {}\n",
    "\n",
    "        for j in range(len(y_kmeans)):\n",
    "            if y_kmeans[j] == i:\n",
    "                my_dict[j] = distance.euclidean(\n",
    "                    kmeans.cluster_centers_[i], sen_vector[j])\n",
    "        my_list.append(min(my_dict, key=my_dict.get))\n",
    "\n",
    "    return my_list"
   ]
  },
  {
   "source": [
    "## K-Medoids"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performKMedoids(text, cluster):\n",
    "    sentence = sent_tokenize(text)\n",
    "    corpus = defineCorpus(sentence)\n",
    "    sentenceVector = vectorize(corpus)\n",
    "    result = kMedoidsClustering(sentenceVector, cluster)\n",
    "    summary = []\n",
    "    for i in sorted(result):\n",
    "        summary.append(sentence[i])\n",
    "    summary = \" \".join(summary)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def kMedoidsClustering(sen_vector, cluster):\n",
    "    n_clusters = cluster\n",
    "    kMedoids = KMedoids(n_clusters, random_state=43, method=\"pam\")\n",
    "    y_kmedoids = kMedoids.fit_predict(sen_vector)\n",
    "\n",
    "    # finding and printing the nearest sentence vector from cluster centroid\n",
    "    my_list = []\n",
    "    for i in range(n_clusters):\n",
    "        my_dict = {}\n",
    "\n",
    "        for j in range(len(y_kmedoids)):\n",
    "            if y_kmedoids[j] == i:\n",
    "                my_dict[j] = distance.euclidean(\n",
    "                    kMedoids.labels_[i], sen_vector[j])\n",
    "        my_list.append(min(my_dict, key=my_dict.get))\n",
    "\n",
    "    return my_list"
   ]
  },
  {
   "source": [
    "## Cosine Similarities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarities(text):\n",
    "    # tokenize sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    #set stop words\n",
    "    stops = list(set(stopwords.words('english'))) + list(punctuation)\n",
    "    \n",
    "    #vectorize sentences and remove stop words\n",
    "    vectorizer = TfidfVectorizer(stop_words = stops)\n",
    "    #transform using TFIDF vectorizer\n",
    "    trsfm=vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    #creat df for input article\n",
    "    text_df = pd.DataFrame(trsfm.toarray(),columns=vectorizer.get_feature_names(),index=sentences)\n",
    "    \n",
    "    #declare how many sentences to use in summary\n",
    "    num_sentences = text_df.shape[0]\n",
    "    num_summary_sentences = int(np.ceil(num_sentences**.5))\n",
    "        \n",
    "    #find cosine similarity for all sentence pairs\n",
    "    similarities = cosine_similarity(trsfm, trsfm)\n",
    "    \n",
    "    #create list to hold avg cosine similarities for each sentence\n",
    "    avgs = []\n",
    "    for i in similarities:\n",
    "        avgs.append(i.mean())\n",
    "     \n",
    "    #find index values of the sentences to be used for summary\n",
    "    top_idx = np.argsort(avgs)[-num_summary_sentences:]\n",
    "    \n",
    "    return top_idx\n",
    "\n",
    "def performCosineSimilarity(text):\n",
    "    #find sentences to extract for summary\n",
    "    sents_for_sum = find_similarities(text)\n",
    "    #sort the sentences\n",
    "    sort = sorted(sents_for_sum)\n",
    "    #display which sentences have been selected\n",
    "    # print(sort)\n",
    "    \n",
    "    sent_list = sent_tokenize(text)\n",
    "    #print number of sentences in full article\n",
    "    # print(len(sent_list))\n",
    "    \n",
    "    \n",
    "    #extract the selected sentences from the original text\n",
    "    sents = []\n",
    "    for i in sort:\n",
    "        sents.append(sent_list[i].replace('\\n', ''))\n",
    "    \n",
    "    #join sentences together for final output\n",
    "    summary = ' '.join(sents)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../text_data/input3.txt'\n",
    "file = open(file , 'r')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nOriginal Text: \n\n Sentence representation at the semantic level is a challenging task for natural language processing and Artificial Intelligence.\nDespite the advances in word embeddings (i.e. word vector representations), capturing sentence meaning is an open question due to complexities of semantic interactions among words. In this paper, we present an embedding method, which is aimed at learning unsupervised sentence representations from unlabeled text. We propose an unsupervised method that models a sentence as a weighted series of word embeddings. The weights of the series are fitted by using Shannon’s Mutual Information (MI) among words, sentences and the corpus. In fact, the Term Frequency?Inverse Document Frequency transform (TF?IDF) is a reliable estimate of such MI. Our method offers advantages over existing ones: identifiable modules, short-term training, online inference of (unseen) sentence representations, as well as independence from domain, external knowledge and linguistic annotation resour- ces. Results showed that our model, despite its concreteness and low computational cost, was competitive with the state of the art in well-known Semantic Textual Similarity (STS) tasks. Ó\n\nTF-IDF Applied: \n\n Despite the advances in word embeddings (i.e. word vector representations), capturing sentence meaning is an open question due to complexities of semantic interactions among words. In this paper, we present an embedding method, which is aimed at learning unsupervised sentence representations from unlabeled text. We propose an unsupervised method that models a sentence as a weighted series of word embeddings. Our method offers advantages over existing ones: identifiable modules, short-term training, online inference of (unseen) sentence representations, as well as independence from domain, external knowledge and linguistic annotation resour- ces. Results showed that our model, despite its concreteness and low computational cost, was competitive with the state of the art in well-known Semantic Textual Similarity (STS) tasks.\n"
     ]
    }
   ],
   "source": [
    "tfidfSummary = buildTfIdfSummary(text, 60)\n",
    "print('\\nOriginal Text: \\n\\n', text)\n",
    "print('\\nTF-IDF Applied: \\n\\n', tfidfSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "n_samples=4 should be >= n_clusters=7.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-73666e34efd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkmeansSummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperformKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidfSummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcosineSimilaritySummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperformCosineSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeansSummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nK-Means Applied: \\n\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeansSummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nCosine Similarities Applied: \\n\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosineSimilaritySummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-2f8b09716e5b>\u001b[0m in \u001b[0;36mperformKMeans\u001b[0;34m(text, cluster)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefineCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msentenceVector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkMeansClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-2f8b09716e5b>\u001b[0m in \u001b[0;36mkMeansClustering\u001b[0;34m(sen_vector, cluster)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m43\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0my_kmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# finding and printing the nearest sentence vector from cluster centroid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1075\u001b[0m             \u001b[0mIndex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0mbelongs\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \"\"\"\n\u001b[0;32m-> 1077\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    982\u001b[0m                                 accept_large_sparse=False)\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_check_params\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;31m# n_clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             raise ValueError(f\"n_samples={X.shape[0]} should be >= \"\n\u001b[0m\u001b[1;32m    813\u001b[0m                              f\"n_clusters={self.n_clusters}.\")\n\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_samples=4 should be >= n_clusters=7."
     ]
    }
   ],
   "source": [
    "kmeansSummary = performKMeans(tfidfSummary, 7)\n",
    "cosineSimilaritySummary = performCosineSimilarity(kmeansSummary)\n",
    "\n",
    "print('\\nK-Means Applied: \\n\\n', kmeansSummary)\n",
    "print('\\nCosine Similarities Applied: \\n\\n', cosineSimilaritySummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The number of medoids (7) must be less than the number of samples 4.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-a88ed6923e7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkmedoidsSummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperformKMedoids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidfSummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcosineSimilaritySummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperformCosineSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmedoidsSummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nK-Medoids Result: \\n\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmedoidsSummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nCosine Similarities Result: \\n\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosineSimilaritySummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-a43262573e96>\u001b[0m in \u001b[0;36mperformKMedoids\u001b[0;34m(text, cluster)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefineCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msentenceVector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkMedoidsClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-a43262573e96>\u001b[0m in \u001b[0;36mkMedoidsClustering\u001b[0;34m(sen_vector, cluster)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mkMedoids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMedoids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m43\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0my_kmedoids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkMedoids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# finding and printing the nearest sentence vector from cluster centroid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;31m# non-optimized default implementation; override when a better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;31m# method is possible for a given clustering algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn_extra/cluster/_k_medoids.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    191\u001b[0m                 \u001b[0;34m\"The number of medoids (%d) must be less \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;34m\"than the number of samples %d.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The number of medoids (7) must be less than the number of samples 4."
     ]
    }
   ],
   "source": [
    "\n",
    "kmedoidsSummary = performKMedoids(tfidfSummary, 7)\n",
    "cosineSimilaritySummary = performCosineSimilarity(kmedoidsSummary)\n",
    "\n",
    "print('\\nK-Medoids Result: \\n\\n', kmedoidsSummary)\n",
    "print('\\nCosine Similarities Result: \\n\\n', cosineSimilaritySummary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd071f37d437d21414a61d0fdaaf400fc7c047edf0caeff3cf8253477f0f1ee4d19",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}