{
 "cells": [
  {
   "source": [
    "## Initiate Text Sample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = '../text_data/input.txt'\n",
    "# file = open(file , 'r')\n",
    "# text = file.read()"
   ]
  },
  {
   "source": [
    "# Performing TD-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing Modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import operator\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances_argmin_min\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial import distance\n",
    "from string import punctuation"
   ]
  },
  {
   "source": [
    "### Downloading libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/hackintosh/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/hackintosh/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/hackintosh/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/hackintosh/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stopwords = set(stopwords.words('english'))\n",
    "wordlemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(words):\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "       lemmatized_words.append(wordlemmatizer.lemmatize(word))\n",
    "    return lemmatized_words\n",
    "def stem_words(words):\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "       stemmed_words.append(PorterStemmer().stem(word))\n",
    "    return stemmed_words\n",
    "def remove_special_characters(text):\n",
    "    regex = r'[^a-zA-Z0-9\\s]'\n",
    "    text = re.sub(regex,'',text)\n",
    "    return text\n",
    "def freq(words):\n",
    "    words = [word.lower() for word in words]\n",
    "    dict_freq = {}\n",
    "    words_unique = []\n",
    "    for word in words:\n",
    "       if word not in words_unique:\n",
    "           words_unique.append(word)\n",
    "    for word in words_unique:\n",
    "       dict_freq[word] = words.count(word)\n",
    "    return dict_freq\n",
    "def pos_tagging(text):\n",
    "    pos_tag = nltk.pos_tag(text.split())\n",
    "    pos_tagged_noun_verb = []\n",
    "    for word,tag in pos_tag:\n",
    "        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n",
    "             pos_tagged_noun_verb.append(word)\n",
    "    return pos_tagged_noun_verb\n",
    "def tf_score(word,sentence):\n",
    "    freq_sum = 0\n",
    "    word_frequency_in_sentence = 0\n",
    "    len_sentence = len(sentence)\n",
    "    for word_in_sentence in sentence.split():\n",
    "        if word == word_in_sentence:\n",
    "            word_frequency_in_sentence = word_frequency_in_sentence + 1\n",
    "    tf =  word_frequency_in_sentence/ len_sentence\n",
    "    return tf\n",
    "def idf_score(no_of_sentences,word,sentences):\n",
    "    no_of_sentence_containing_word = 0\n",
    "    # print('WORDS', word)\n",
    "    for sentence in sentences:\n",
    "        sentence = remove_special_characters(str(sentence))\n",
    "        sentence = re.sub(r'\\d+', '', sentence)\n",
    "        sentence = sentence.split()\n",
    "        sentence = [word for word in sentence if word.lower() not in Stopwords and len(word)>1]\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "        sentence = [wordlemmatizer.lemmatize(word) for word in sentence]\n",
    "        if word in sentence:\n",
    "            no_of_sentence_containing_word = no_of_sentence_containing_word + 1\n",
    "    idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\n",
    "    return idf\n",
    "def tf_idf_score(tf,idf):\n",
    "    return tf*idf\n",
    "def word_tfidf(dict_freq,word,sentences,sentence):\n",
    "    word_tfidf = []\n",
    "    tf = tf_score(word,sentence)\n",
    "    idf = idf_score(len(sentences),word,sentences)\n",
    "    tf_idf = tf_idf_score(tf,idf)\n",
    "    return tf_idf\n",
    "def sentence_importance(sentence,dict_freq,sentences):\n",
    "     sentence_score = 0\n",
    "     sentence = remove_special_characters(str(sentence)) \n",
    "     sentence = re.sub(r'\\d+', '', sentence)\n",
    "     pos_tagged_sentence = [] \n",
    "     no_of_sentences = len(sentences)\n",
    "     pos_tagged_sentence = pos_tagging(sentence)\n",
    "     for word in pos_tagged_sentence:\n",
    "          if word.lower() not in Stopwords and word not in Stopwords and len(word)>1: \n",
    "                word = word.lower()\n",
    "                word = wordlemmatizer.lemmatize(word)\n",
    "                sentence_score = sentence_score + word_tfidf(dict_freq,word,sentences,sentence)\n",
    "     return sentence_score\n",
    "# file = '../text_data/input.txt'\n",
    "# file = open(file , 'r')\n",
    "# text = file.read()\n",
    "def buildTfIdfSummary(text):\n",
    "    summary = []\n",
    "    sentence_no = []\n",
    "\n",
    "    tokenized_sentence = sent_tokenize(text.rstrip('\\n'))\n",
    "    text = remove_special_characters(str(text))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    tokenized_words_with_stopwords = word_tokenize(text)\n",
    "    tokenized_words = [word for word in tokenized_words_with_stopwords if word not in Stopwords]\n",
    "    tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "    tokenized_words = [word.lower() for word in tokenized_words]\n",
    "    tokenized_words = lemmatize_words(tokenized_words)\n",
    "\n",
    "\n",
    "    sentence_with_importance = getSentenceImportance(tokenized_sentence, tokenized_words)\n",
    "\n",
    "    sentence_with_importance = sorted(sentence_with_importance.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "    addSentenceNumber(tokenized_sentence, sentence_with_importance, sentence_no)\n",
    "    \n",
    "    sentence_no.sort()\n",
    "\n",
    "    summary = assembleWords(tokenized_sentence, sentence_no)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def getSentenceImportance(tokenized_sentence, tokenized_words):\n",
    "    sentence_with_importance = {}\n",
    "    c = 1\n",
    "    word_freq = freq(tokenized_words)\n",
    "    for sent in tokenized_sentence:\n",
    "        sentenceimp = sentence_importance(sent,word_freq,tokenized_sentence)\n",
    "        sentence_with_importance[c] = sentenceimp\n",
    "        c += 1\n",
    "    return sentence_with_importance\n",
    "\n",
    "\n",
    "def addSentenceNumber(tokenized_sentence, sentence_with_importance, sentence_no):\n",
    "    cnt = 0\n",
    "    no_of_sentences = int((50 * len(tokenized_sentence))/100)\n",
    "    for word_prob in sentence_with_importance:\n",
    "        if cnt < no_of_sentences:\n",
    "            sentence_no.append(word_prob[0])\n",
    "            cnt = cnt+1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "def assembleWords(tokenized_sentence, sentence_no):\n",
    "    cnt = 1\n",
    "    summary = []\n",
    "    for sentence in tokenized_sentence:\n",
    "        if cnt in sentence_no:\n",
    "            summary.append(sentence)\n",
    "        cnt = cnt+1\n",
    "    \n",
    "    summary = \" \".join(summary)\n",
    "    return summary\n"
   ]
  },
  {
   "source": [
    "## Word Vectorizing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineCorpus(sentence):\n",
    "    corpus = []\n",
    "    for i in range(len(sentence)):\n",
    "        sen = re.sub('[^a-zA-Z]', \" \", sentence[i])\n",
    "        sen = sen.lower()\n",
    "        sen = sen.split()\n",
    "        sen = ' '.join([i for i in sen if i not in stopwords.words('english')])\n",
    "        corpus.append(sen)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def vectorize(corpus):\n",
    "\n",
    "    # creating word vectors\n",
    "    n = 300\n",
    "    all_words = [i.split() for i in corpus]\n",
    "    model = Word2Vec(all_words, min_count=1, size=n)\n",
    "\n",
    "    # creating sentence vectors\n",
    "    sen_vector = []\n",
    "    for i in corpus:\n",
    "        plus = 0\n",
    "        for j in i.split():\n",
    "            plus += model.wv[j]\n",
    "        plus = plus/len(plus)\n",
    "        sen_vector.append(plus)\n",
    "    return sen_vector"
   ]
  },
  {
   "source": [
    "## K-Means"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performKMeans(text, cluster):\n",
    "    sentence = sent_tokenize(text)\n",
    "    corpus = defineCorpus(sentence)\n",
    "    sentenceVector = vectorize(corpus)\n",
    "    result = kMeansClustering(sentenceVector, cluster)\n",
    "    summary = []\n",
    "    for i in sorted(result):\n",
    "        summary.append(sentence[i])\n",
    "    summary = \" \".join(summary)\n",
    "    return summary\n",
    "\n",
    "# performing k-means\n",
    "\n",
    "\n",
    "def kMeansClustering(sen_vector, cluster):\n",
    "    n_clusters = cluster\n",
    "    kmeans = KMeans(n_clusters, init='k-means++', random_state=43)\n",
    "    y_kmeans = kmeans.fit_predict(sen_vector)\n",
    "\n",
    "    # finding and printing the nearest sentence vector from cluster centroid\n",
    "    my_list = []\n",
    "    for i in range(n_clusters):\n",
    "        my_dict = {}\n",
    "\n",
    "        for j in range(len(y_kmeans)):\n",
    "            if y_kmeans[j] == i:\n",
    "                my_dict[j] = distance.euclidean(\n",
    "                    kmeans.cluster_centers_[i], sen_vector[j])\n",
    "        my_list.append(min(my_dict, key=my_dict.get))\n",
    "\n",
    "    return my_list"
   ]
  },
  {
   "source": [
    "## K-Medoids"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performKMedoids(text, cluster):\n",
    "    sentence = sent_tokenize(text)\n",
    "    corpus = defineCorpus(sentence)\n",
    "    sentenceVector = vectorize(corpus)\n",
    "    result = kMedoidsClustering(sentenceVector, cluster)\n",
    "    summary = []\n",
    "    for i in sorted(result):\n",
    "        summary.append(sentence[i])\n",
    "    summary = \" \".join(summary)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def kMedoidsClustering(sen_vector, cluster):\n",
    "    n_clusters = cluster\n",
    "    kMedoids = KMedoids(n_clusters, random_state=0, method=\"pam\")\n",
    "    y_kmedoids = kMedoids.fit_predict(sen_vector)\n",
    "\n",
    "    # finding and printing the nearest sentence vector from cluster centroid\n",
    "    my_list = []\n",
    "    for i in range(n_clusters):\n",
    "        my_dict = {}\n",
    "\n",
    "        for j in range(len(y_kmedoids)):\n",
    "            if y_kmedoids[j] == i:\n",
    "                my_dict[j] = distance.euclidean(\n",
    "                    kMedoids.labels_[i], sen_vector[j])\n",
    "        my_list.append(min(my_dict, key=my_dict.get))\n",
    "\n",
    "    return my_list"
   ]
  },
  {
   "source": [
    "## Cosine Similarities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarities(text):\n",
    "    # tokenize sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    #set stop words\n",
    "    stops = list(set(stopwords.words('english'))) + list(punctuation)\n",
    "    \n",
    "    #vectorize sentences and remove stop words\n",
    "    vectorizer = TfidfVectorizer(stop_words = stops)\n",
    "    #transform using TFIDF vectorizer\n",
    "    trsfm=vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    #creat df for input article\n",
    "    text_df = pd.DataFrame(trsfm.toarray(),columns=vectorizer.get_feature_names(),index=sentences)\n",
    "    \n",
    "    #declare how many sentences to use in summary\n",
    "    num_sentences = text_df.shape[0]\n",
    "    num_summary_sentences = int(np.ceil(num_sentences**.5))\n",
    "        \n",
    "    #find cosine similarity for all sentence pairs\n",
    "    similarities = cosine_similarity(trsfm, trsfm)\n",
    "    \n",
    "    #create list to hold avg cosine similarities for each sentence\n",
    "    avgs = []\n",
    "    for i in similarities:\n",
    "        avgs.append(i.mean())\n",
    "     \n",
    "    #find index values of the sentences to be used for summary\n",
    "    top_idx = np.argsort(avgs)[-num_summary_sentences:]\n",
    "    \n",
    "    return top_idx\n",
    "\n",
    "def performCosineSimilarity(text):\n",
    "    #find sentences to extract for summary\n",
    "    sents_for_sum = find_similarities(text)\n",
    "    #sort the sentences\n",
    "    sort = sorted(sents_for_sum)\n",
    "    #display which sentences have been selected\n",
    "    # print(sort)\n",
    "    \n",
    "    sent_list = sent_tokenize(text)\n",
    "    #print number of sentences in full article\n",
    "    # print(len(sent_list))\n",
    "    \n",
    "    \n",
    "    #extract the selected sentences from the original text\n",
    "    sents = []\n",
    "    for i in sort:\n",
    "        sents.append(sent_list[i].replace('\\n', ''))\n",
    "    \n",
    "    #join sentences together for final output\n",
    "    summary = ' '.join(sents)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../text_data/input3.txt'\n",
    "file = open(file , 'r')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nOriginal Text: \n\n Sentence representation at the semantic level is a challenging task for natural language processing and Artificial Intelligence.\nDespite the advances in word embeddings (i.e. word vector representations), capturing sentence meaning is an open question due to complexities of semantic interactions among words. In this paper, we present an embedding method, which is aimed at learning unsupervised sentence representations from unlabeled text. We propose an unsupervised method that models a sentence as a weighted series of word embeddings. The weights of the series are fitted by using Shannon’s Mutual Information (MI) among words, sentences and the corpus. In fact, the Term Frequency?Inverse Document Frequency transform (TF?IDF) is a reliable estimate of such MI. Our method offers advantages over existing ones: identifiable modules, short-term training, online inference of (unseen) sentence representations, as well as independence from domain, external knowledge and linguistic annotation resour- ces. Results showed that our model, despite its concreteness and low computational cost, was competitive with the state of the art in well-known Semantic Textual Similarity (STS) tasks. Ó\n\nTF-IDF Applied: \n\n Despite the advances in word embeddings (i.e. word vector representations), capturing sentence meaning is an open question due to complexities of semantic interactions among words. In this paper, we present an embedding method, which is aimed at learning unsupervised sentence representations from unlabeled text. We propose an unsupervised method that models a sentence as a weighted series of word embeddings. Our method offers advantages over existing ones: identifiable modules, short-term training, online inference of (unseen) sentence representations, as well as independence from domain, external knowledge and linguistic annotation resour- ces.\n"
     ]
    }
   ],
   "source": [
    "tfidfSummary = buildTfIdfSummary(text)\n",
    "print('\\nOriginal Text: \\n\\n', text)\n",
    "print('\\nTF-IDF Applied: \\n\\n', tfidfSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nCosine Similarities Applied: \n\n The long time between project start and go-live causes a gap between initial solution blueprint and actual user requirements in the end of the project. It presents a case study provided in a large telecommunica- tions company (20K employees) and the results of a pilot research provided in the three large companies: telecommunications, digital, and insurance.\n\nK-Means Applied: \n\n Today organizations require better use of data and analytics to support their business deci- sions. The long time between project start and go-live causes a gap between initial solution blueprint and actual user requirements in the end of the project. It presents a case study provided in a large telecommunica- tions company (20K employees) and the results of a pilot research provided in the three large companies: telecommunications, digital, and insurance.\n"
     ]
    }
   ],
   "source": [
    "kmeansSummary = performKMeans(tfidfSummary, 3)\n",
    "cosineSimilaritySummary = performCosineSimilarity(kmeansSummary)\n",
    "\n",
    "print('\\nCosine Similarities Applied: \\n\\n', cosineSimilaritySummary)\n",
    "print('\\nK-Means Applied: \\n\\n', kmeansSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nK-Medoids Result: \n\n Internet power and business trend changes have provided a broad term for data analytics – Big Data. The long time between project start and go-live causes a gap between initial solution blueprint and actual user requirements in the end of the project. It presents a case study provided in a large telecommunica- tions company (20K employees) and the results of a pilot research provided in the three large companies: telecommunications, digital, and insurance.\n\nCosine Similarities Result: \n\n Internet power and business trend changes have provided a broad term for data analytics – Big Data. It presents a case study provided in a large telecommunica- tions company (20K employees) and the results of a pilot research provided in the three large companies: telecommunications, digital, and insurance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kmedoidsSummary = performKMedoids(tfidfSummary, 3)\n",
    "cosineSimilaritySummary = performCosineSimilarity(kmedoidsSummary)\n",
    "\n",
    "print('\\nK-Medoids Result: \\n\\n', kmedoidsSummary)\n",
    "print('\\nCosine Similarities Result: \\n\\n', cosineSimilaritySummary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd071f37d437d21414a61d0fdaaf400fc7c047edf0caeff3cf8253477f0f1ee4d19",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}