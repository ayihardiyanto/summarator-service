{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd071f37d437d21414a61d0fdaaf400fc7c047edf0caeff3cf8253477f0f1ee4d19",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Import Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/hackintosh/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n"
   ]
  },
  {
   "source": [
    "## Find Similarities Function\n",
    "This function will determine which sentences to extract from the article's text by finding the cosine similarity between all tf-idf transformed sentences. The extracted sentences will have the highest average cosine similarity to the remaining sentences. By doing this, the summary should include sentences that show the highest importance to the article.\n",
    "\n",
    "> Purnamasari (2019), Cosine Similarity models term into vectors and calculate the cosine value of two tokens. The token used\n",
    "can be in the form of words, paragraphs, or even the entire source-text. Sorted cosine value will indicate\n",
    "the similarity among terms. High scores will indicate a large degree of similarity."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarities(text):\n",
    "    # tokenize sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    #set stop words\n",
    "    stops = list(set(stopwords.words('english'))) + list(punctuation)\n",
    "    \n",
    "    #vectorize sentences and remove stop words\n",
    "    vectorizer = TfidfVectorizer(stop_words = stops)\n",
    "    #transform using TFIDF vectorizer\n",
    "    trsfm=vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    #creat df for input article\n",
    "    text_df = pd.DataFrame(trsfm.toarray(),columns=vectorizer.get_feature_names(),index=sentences)\n",
    "    \n",
    "    #declare how many sentences to use in summary\n",
    "    num_sentences = text_df.shape[0]\n",
    "    num_summary_sentences = int(np.ceil(num_sentences**.5))\n",
    "        \n",
    "    #find cosine similarity for all sentence pairs\n",
    "    similarities = cosine_similarity(trsfm, trsfm)\n",
    "    \n",
    "    #create list to hold avg cosine similarities for each sentence\n",
    "    avgs = []\n",
    "    for i in similarities:\n",
    "        avgs.append(i.mean())\n",
    "     \n",
    "    #find index values of the sentences to be used for summary\n",
    "    top_idx = np.argsort(avgs)[-num_summary_sentences:]\n",
    "    \n",
    "    return top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_summary(text):\n",
    "    #find sentences to extract for summary\n",
    "    sents_for_sum = find_similarities(text)\n",
    "    #sort the sentences\n",
    "    sort = sorted(sents_for_sum)\n",
    "    #display which sentences have been selected\n",
    "    # print(sort)\n",
    "    \n",
    "    sent_list = sent_tokenize(text)\n",
    "    #print number of sentences in full article\n",
    "    # print(len(sent_list))\n",
    "    \n",
    "    \n",
    "    #extract the selected sentences from the original text\n",
    "    sents = []\n",
    "    for i in sort:\n",
    "        sents.append(sent_list[i].replace('\\n', ''))\n",
    "    \n",
    "    #join sentences together for final output\n",
    "    summary = ' '.join(sents)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cosine similarity: \n\nToday organizations require better use of data and analytics to support their business deci- sions. Internet power and business trend changes have provided a broad term for data analytics â€“ Big Data. It helps to provide not only reliable data and good analysis but at the same time to optimize the process and increase added value.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file = '../text_data/summary.txt'\n",
    "file = open(file , 'r')\n",
    "text = file.read()\n",
    "\n",
    "print('Cosine similarity: \\n')\n",
    "print(build_summary(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}