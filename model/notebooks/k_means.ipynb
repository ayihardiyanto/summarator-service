{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "#nltk.download('punkt')   # one time execution\n",
    "#nltk.download('stopwords')  # one time execution\n",
    "from nltk.corpus import stopwords\n",
    "# \n",
    "#sentence tokenization\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentence = sent_tokenize(summary)\n",
    "\n",
    "# print('SENTENCE', sentence)\n",
    "\n",
    "# cleaning the sentences\n",
    "\n",
    "# corpus = summary\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(sentence)):\n",
    "    sen = re.sub('[^a-zA-Z]', \" \", sentence[i])  \n",
    "    sen = sen.lower()                            \n",
    "    sen=sen.split()                         \n",
    "    sen = ' '.join([i for i in sen if i not in stopwords.words('english')])\n",
    "#     print('SEN', sen)\n",
    "    corpus.append(sen)\n",
    "    \n",
    "# print('CORPUS', corpus)\n",
    "    \n",
    "\n",
    "#creating word vectors\n",
    "\n",
    "n=300\n",
    "all_words = [i.split() for i in corpus]\n",
    "model = Word2Vec(all_words, min_count=1,size= n)\n",
    "\n",
    "# creating sentence vectors\n",
    "\n",
    "sen_vector=[]\n",
    "for i in corpus:\n",
    "    \n",
    "    plus=0\n",
    "    for j in i.split():\n",
    "        plus+=model.wv[j]\n",
    "    plus = plus/len(plus)\n",
    "    \n",
    "    sen_vector.append(plus)\n",
    "    \n",
    "#performing k-means  \n",
    "    \n",
    "n_clusters = 3\n",
    "kmeans = KMeans(3, init = 'k-means++', random_state = 43)\n",
    "y_kmeans = kmeans.fit_predict(sen_vector)\n",
    "\n",
    "#finding and printing the nearest sentence vector from cluster centroid\n",
    "\n",
    "\n",
    "my_list=[]\n",
    "for i in range(n_clusters):\n",
    "    my_dict={}\n",
    "    \n",
    "    for j in range(len(y_kmeans)):\n",
    "        \n",
    "        if y_kmeans[j]==i:\n",
    "            my_dict[j] =  distance.euclidean(kmeans.cluster_centers_[i],sen_vector[j])\n",
    "    min_distance = min(my_dict.values())\n",
    "    my_list.append(min(my_dict, key=my_dict.get))\n",
    "\n",
    "                            \n",
    "# print(my_list)\n",
    "# print(y_kmeans)\n",
    "print('TF-IDF Result:\\n\\n', summary)\n",
    "print('----------------------------------------------------------------\\n')\n",
    "\n",
    "print('K-Means Applied:\\n')\n",
    "for i in sorted(my_list):\n",
    "    print(sentence[i])"
   ]
  }
 ]
}